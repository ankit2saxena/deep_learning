{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Test set', (10000, 28, 28), (10000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000,))\n",
      "('Test set', (10000, 784), (10000,))\n",
      "('Validation set', (10000, 784), (10000,))\n"
     ]
    }
   ],
   "source": [
    "image_height = 28\n",
    "image_width = 28\n",
    "num_labels = 10\n",
    "\n",
    "train_dataset = train_dataset.reshape((-1, image_height * image_width)).astype(np.float32)\n",
    "test_dataset = test_dataset.reshape((-1, image_height * image_width)).astype(np.float32)\n",
    "valid_dataset = valid_dataset.reshape((-1, image_height * image_width)).astype(np.float32)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_image(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = 'Greys', interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAFfCAYAAADptc+BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvgAB8QUIiC9AQHwBAuILEPgfrHOf9mMneDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54e6b5c910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = 10000\n",
    "batch_size = 50\n",
    "epochs = 101\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :], shape= (10000, image_width * image_height) )\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset], shape = (None))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([image_width * image_width, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    train_logits = tf.add(tf.matmul(tf_train_dataset, weights), biases)\n",
    "    test_logits = tf.add(tf.matmul(tf_test_dataset, weights), biases)\n",
    "    valid_logits = tf.add(tf.matmul(tf_valid_dataset, weights), biases)\n",
    "    pass\n",
    "\n",
    "with tf.name_scope('loss_train'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = train_logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    pass\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy_valid = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = valid_logits, labels=valid_labels)\n",
    "    loss_valid = tf.reduce_mean(xentropy_valid, name = 'loss')\n",
    "    loss_summary_valid = tf.summary.scalar('log_loss', loss_valid)\n",
    "    pass\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.4)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    pass\n",
    "\n",
    "with tf.name_scope('eval'):    \n",
    "    train_predicted = tf.nn.in_top_k(train_logits, tf_train_labels, 1)\n",
    "    test_predicted = tf.nn.in_top_k(test_logits, test_labels, 1)\n",
    "    valid_predicted = tf.nn.in_top_k(valid_logits, valid_labels, 1)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(train_predicted, tf.float32))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(test_predicted, tf.float32))\n",
    "    valid_accuracy = tf.reduce_mean(tf.cast(valid_predicted, tf.float32))\n",
    "    \n",
    "    accuracy_summary_valid = tf.summary.scalar('accuracy', valid_accuracy)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter('/tmp/notmnist/notmnist_dnn-GD', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 17.835, Train Accuracy: 0.100\n",
      "Test Accuracy: 0.112, Validation Accuracy: 0.116\n",
      "\n",
      "\n",
      "Iteration 5, Train Loss: 8.872, Train Accuracy: 0.234\n",
      "Test Accuracy: 0.273, Validation Accuracy: 0.264\n",
      "\n",
      "\n",
      "Iteration 10, Train Loss: 5.687, Train Accuracy: 0.407\n",
      "Test Accuracy: 0.470, Validation Accuracy: 0.430\n",
      "\n",
      "\n",
      "Iteration 15, Train Loss: 4.617, Train Accuracy: 0.503\n",
      "Test Accuracy: 0.562, Validation Accuracy: 0.511\n",
      "\n",
      "\n",
      "Iteration 20, Train Loss: 4.109, Train Accuracy: 0.552\n",
      "Test Accuracy: 0.607, Validation Accuracy: 0.550\n",
      "\n",
      "\n",
      "Iteration 25, Train Loss: 3.795, Train Accuracy: 0.585\n",
      "Test Accuracy: 0.637, Validation Accuracy: 0.579\n",
      "\n",
      "\n",
      "Iteration 30, Train Loss: 3.573, Train Accuracy: 0.605\n",
      "Test Accuracy: 0.659, Validation Accuracy: 0.600\n",
      "\n",
      "\n",
      "Iteration 35, Train Loss: 3.402, Train Accuracy: 0.623\n",
      "Test Accuracy: 0.677, Validation Accuracy: 0.614\n",
      "\n",
      "\n",
      "Iteration 40, Train Loss: 3.264, Train Accuracy: 0.636\n",
      "Test Accuracy: 0.692, Validation Accuracy: 0.629\n",
      "\n",
      "\n",
      "Iteration 45, Train Loss: 3.149, Train Accuracy: 0.647\n",
      "Test Accuracy: 0.705, Validation Accuracy: 0.639\n",
      "\n",
      "\n",
      "Iteration 50, Train Loss: 3.051, Train Accuracy: 0.655\n",
      "Test Accuracy: 0.715, Validation Accuracy: 0.648\n",
      "\n",
      "\n",
      "Iteration 55, Train Loss: 2.964, Train Accuracy: 0.662\n",
      "Test Accuracy: 0.725, Validation Accuracy: 0.655\n",
      "\n",
      "\n",
      "Iteration 60, Train Loss: 2.888, Train Accuracy: 0.670\n",
      "Test Accuracy: 0.731, Validation Accuracy: 0.663\n",
      "\n",
      "\n",
      "Iteration 65, Train Loss: 2.819, Train Accuracy: 0.677\n",
      "Test Accuracy: 0.738, Validation Accuracy: 0.669\n",
      "\n",
      "\n",
      "Iteration 70, Train Loss: 2.757, Train Accuracy: 0.684\n",
      "Test Accuracy: 0.743, Validation Accuracy: 0.675\n",
      "\n",
      "\n",
      "Iteration 75, Train Loss: 2.700, Train Accuracy: 0.691\n",
      "Test Accuracy: 0.748, Validation Accuracy: 0.680\n",
      "\n",
      "\n",
      "Iteration 80, Train Loss: 2.648, Train Accuracy: 0.693\n",
      "Test Accuracy: 0.753, Validation Accuracy: 0.683\n",
      "\n",
      "\n",
      "Iteration 85, Train Loss: 2.600, Train Accuracy: 0.698\n",
      "Test Accuracy: 0.758, Validation Accuracy: 0.687\n",
      "\n",
      "\n",
      "Iteration 90, Train Loss: 2.556, Train Accuracy: 0.702\n",
      "Test Accuracy: 0.762, Validation Accuracy: 0.690\n",
      "\n",
      "\n",
      "Iteration 95, Train Loss: 2.514, Train Accuracy: 0.706\n",
      "Test Accuracy: 0.766, Validation Accuracy: 0.693\n",
      "\n",
      "\n",
      "Iteration 100, Train Loss: 2.475, Train Accuracy: 0.710\n",
      "Test Accuracy: 0.768, Validation Accuracy: 0.696\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        _, accuracy_val, loss_val = sess.run([training_op, train_accuracy, loss])\n",
    "        test_acc_val = sess.run(test_accuracy)\n",
    "        \n",
    "        vaccuracy, vloss, vaccuracy_summary, vloss_summary = sess.run([valid_accuracy, loss_valid, accuracy_summary_valid, loss_summary_valid])\n",
    "        file_writer.add_summary(vaccuracy_summary, epoch)\n",
    "        file_writer.add_summary(vloss_summary, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print ('Iteration %d, Train Loss: %.3f, Train Accuracy: %.3f' % (epoch, loss_val, accuracy_val))\n",
    "            print ('Test Accuracy: %.3f, Validation Accuracy: %.3f' % (test_acc_val, vaccuracy))\n",
    "            print '\\n'\n",
    "        pass\n",
    "    save_path = saver.save(sess, \"./model1.ckpt\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 101\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = [batch_size, image_width * image_height])\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape = (batch_size))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([image_width * image_width, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    train_logits = tf.add(tf.matmul(tf_train_dataset, weights), biases)\n",
    "    test_logits = tf.add(tf.matmul(tf_test_dataset, weights), biases)\n",
    "    valid_logits = tf.add(tf.matmul(tf_valid_dataset, weights), biases)\n",
    "    pass\n",
    "\n",
    "with tf.name_scope('loss_train'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = train_logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    pass\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy_valid = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = valid_logits, labels=valid_labels)\n",
    "    loss_valid = tf.reduce_mean(xentropy_valid, name = 'loss')\n",
    "    loss_summary_valid = tf.summary.scalar('log_loss', loss_valid)\n",
    "    pass\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.4)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    pass\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    train_predicted = tf.nn.in_top_k(train_logits, tf_train_labels, 1)\n",
    "    test_predicted = tf.nn.in_top_k(test_logits, test_labels, 1)\n",
    "    valid_predicted = tf.nn.in_top_k(valid_logits, valid_labels, 1)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(train_predicted, tf.float32))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(test_predicted, tf.float32))\n",
    "    valid_accuracy = tf.reduce_mean(tf.cast(valid_predicted, tf.float32))\n",
    "    \n",
    "    accuracy_summary_valid = tf.summary.scalar('accuracy', valid_accuracy)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter('/tmp/notmnist/notmnist_dnn-SGD', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Batch Iteration 0, Train Loss: 16.944, Train Accuracy: 0.120\n",
      "Test Accuracy: 0.146, Validation Accuracy: 0.139\n",
      "\n",
      "\n",
      "Mini Batch Iteration 5, Train Loss: 7.925, Train Accuracy: 0.300\n",
      "Test Accuracy: 0.313, Validation Accuracy: 0.282\n",
      "\n",
      "\n",
      "Mini Batch Iteration 10, Train Loss: 5.159, Train Accuracy: 0.400\n",
      "Test Accuracy: 0.437, Validation Accuracy: 0.383\n",
      "\n",
      "\n",
      "Mini Batch Iteration 15, Train Loss: 5.140, Train Accuracy: 0.540\n",
      "Test Accuracy: 0.498, Validation Accuracy: 0.443\n",
      "\n",
      "\n",
      "Mini Batch Iteration 20, Train Loss: 5.359, Train Accuracy: 0.480\n",
      "Test Accuracy: 0.539, Validation Accuracy: 0.481\n",
      "\n",
      "\n",
      "Mini Batch Iteration 25, Train Loss: 3.928, Train Accuracy: 0.620\n",
      "Test Accuracy: 0.592, Validation Accuracy: 0.530\n",
      "\n",
      "\n",
      "Mini Batch Iteration 30, Train Loss: 2.930, Train Accuracy: 0.600\n",
      "Test Accuracy: 0.638, Validation Accuracy: 0.566\n",
      "\n",
      "\n",
      "Mini Batch Iteration 35, Train Loss: 4.845, Train Accuracy: 0.480\n",
      "Test Accuracy: 0.650, Validation Accuracy: 0.580\n",
      "\n",
      "\n",
      "Mini Batch Iteration 40, Train Loss: 2.695, Train Accuracy: 0.660\n",
      "Test Accuracy: 0.678, Validation Accuracy: 0.606\n",
      "\n",
      "\n",
      "Mini Batch Iteration 45, Train Loss: 3.554, Train Accuracy: 0.580\n",
      "Test Accuracy: 0.698, Validation Accuracy: 0.631\n",
      "\n",
      "\n",
      "Mini Batch Iteration 50, Train Loss: 3.546, Train Accuracy: 0.620\n",
      "Test Accuracy: 0.698, Validation Accuracy: 0.618\n",
      "\n",
      "\n",
      "Mini Batch Iteration 55, Train Loss: 2.808, Train Accuracy: 0.640\n",
      "Test Accuracy: 0.711, Validation Accuracy: 0.637\n",
      "\n",
      "\n",
      "Mini Batch Iteration 60, Train Loss: 3.668, Train Accuracy: 0.600\n",
      "Test Accuracy: 0.717, Validation Accuracy: 0.643\n",
      "\n",
      "\n",
      "Mini Batch Iteration 65, Train Loss: 2.715, Train Accuracy: 0.780\n",
      "Test Accuracy: 0.716, Validation Accuracy: 0.646\n",
      "\n",
      "\n",
      "Mini Batch Iteration 70, Train Loss: 2.919, Train Accuracy: 0.660\n",
      "Test Accuracy: 0.729, Validation Accuracy: 0.654\n",
      "\n",
      "\n",
      "Mini Batch Iteration 75, Train Loss: 2.155, Train Accuracy: 0.720\n",
      "Test Accuracy: 0.730, Validation Accuracy: 0.658\n",
      "\n",
      "\n",
      "Mini Batch Iteration 80, Train Loss: 3.480, Train Accuracy: 0.740\n",
      "Test Accuracy: 0.729, Validation Accuracy: 0.657\n",
      "\n",
      "\n",
      "Mini Batch Iteration 85, Train Loss: 1.845, Train Accuracy: 0.720\n",
      "Test Accuracy: 0.750, Validation Accuracy: 0.678\n",
      "\n",
      "\n",
      "Mini Batch Iteration 90, Train Loss: 3.101, Train Accuracy: 0.720\n",
      "Test Accuracy: 0.749, Validation Accuracy: 0.673\n",
      "\n",
      "\n",
      "Mini Batch Iteration 95, Train Loss: 2.546, Train Accuracy: 0.680\n",
      "Test Accuracy: 0.751, Validation Accuracy: 0.677\n",
      "\n",
      "\n",
      "Mini Batch Iteration 100, Train Loss: 2.675, Train Accuracy: 0.720\n",
      "Test Accuracy: 0.754, Validation Accuracy: 0.678\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_index = np.random.choice(train_labels.shape[0], batch_size)\n",
    "        batch_data, batch_labels = train_dataset[batch_index], train_labels[batch_index]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        \n",
    "        _, accuracy_val, loss_val = sess.run([training_op, train_accuracy, loss], feed_dict=feed_dict)\n",
    "        test_acc_val = sess.run(test_accuracy)\n",
    "        \n",
    "        vaccuracy, vloss, vaccuracy_summary, vloss_summary = sess.run([valid_accuracy, loss_valid, accuracy_summary_valid, loss_summary_valid])\n",
    "        file_writer.add_summary(vaccuracy_summary, epoch)\n",
    "        file_writer.add_summary(vloss_summary, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print ('Mini Batch Iteration %d, Train Loss: %.3f, Train Accuracy: %.3f' % (epoch, loss_val, accuracy_val))\n",
    "            print ('Test Accuracy: %.3f, Validation Accuracy: %.3f' % (test_acc_val, vaccuracy))\n",
    "            print ('\\n')\n",
    "    save_path = saver.save(sess, \"./model2.ckpt\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
