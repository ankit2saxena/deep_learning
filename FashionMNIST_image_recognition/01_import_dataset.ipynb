{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset from Kaggle: https://www.kaggle.com/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract the downloaded zip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filepath = \"../../../datasets/FashionMNIST/\"\n",
    "zip_filename = \"fashionmnist.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(zip_filepath + zip_filename):\n",
    "    with ZipFile(zip_filepath + zip_filename, 'r') as zipObj:\n",
    "        zipObj.extractall(path = \"input/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('input/fashion-mnist_train.csv')\n",
    "test_dataset = pd.read_csv('input/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ROWS, IMG_COLS = 28, 28\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 10\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "input_shape = (IMG_ROWS, IMG_COLS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_dataset.iloc[:, 1:])\n",
    "y = np.array(train_dataset.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 784), (48000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 784), (12000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_dataset.iloc[:, 1:])\n",
    "y_test = np.array(test_dataset.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], IMG_ROWS, IMG_COLS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "                0 : \"T-shirt/top\", \n",
    "                1: \"Trouser\", \n",
    "                2: \"Pullover\", \n",
    "                3: \"Dress\", \n",
    "                4: \"Coat\",\n",
    "                5: \"Sandal\", \n",
    "                6: \"Shirt\", \n",
    "                7: \"Sneaker\", \n",
    "                8: \"Bag\", \n",
    "                9: \"Ankle Boot\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (28, 28, 1)\n",
      "Label:  6 -> Shirt\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape: \", X_train[random_sample].shape)\n",
    "print(\"Label: \", y_train[random_sample], \"->\", class_names[y_train[random_sample]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Log image and view in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/train_data/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.FileWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.reshape(X_train[random_sample], (-1, IMG_ROWS, IMG_COLS, 1))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    file_writer.add_summary(tf.summary.image(\"Training data\", img).eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tensorboard --logdir logs/train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log multiple images and view in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 28, 28, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.reshape(X_train[0: 25], (-1, IMG_ROWS, IMG_COLS, 1))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    file_writer.add_summary(tf.summary.image(\"25 Training Samples\", images, max_outputs = 25).eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y_before_train(y_df):\n",
    "    y_out = tf.keras.utils.to_categorical(y_df, NUM_CLASSES)\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = preprocess_y_before_train(y_train)\n",
    "y_val_cat = preprocess_y_before_train(y_val)\n",
    "y_test_cat = preprocess_y_before_train(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. 1 Layer Hidden NN with ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 1024\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/train_data/\" + \"model1\"\n",
    "tensorboard_callback1 = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 03:58:08.156594 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 03:58:08.160590 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential(name=\"model1\")\n",
    "model1.add(Dense(N_HIDDEN, input_dim = IMG_ROWS * IMG_ROWS, name=\"layer1\"))\n",
    "model1.add(Activation('relu', name=\"layer1_activation\"))\n",
    "model1.add(Dense(NUM_CLASSES, activation='softmax', name=\"layer2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 03:58:08.208596 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 03:58:08.243597 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "layer1_activation (Activatio (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 814,090\n",
      "Trainable params: 814,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 03:58:08.395595 10392 deprecation.py:323] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0812 03:58:08.458592 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0812 03:58:08.558609 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 03:58:08.842592 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0812 03:58:08.845594 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 10s 204us/step - loss: 0.5152 - acc: 0.8172 - val_loss: 0.4582 - val_acc: 0.8259\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.3844 - acc: 0.8619 - val_loss: 0.3586 - val_acc: 0.8733\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 9s 177us/step - loss: 0.3394 - acc: 0.8764 - val_loss: 0.3484 - val_acc: 0.8726\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 9s 190us/step - loss: 0.3143 - acc: 0.8845 - val_loss: 0.3273 - val_acc: 0.8779\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 10s 205us/step - loss: 0.2980 - acc: 0.8900 - val_loss: 0.3394 - val_acc: 0.8761\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 10s 198us/step - loss: 0.2775 - acc: 0.8984 - val_loss: 0.3247 - val_acc: 0.8860\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 11s 228us/step - loss: 0.2666 - acc: 0.9022 - val_loss: 0.3068 - val_acc: 0.8882\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 11s 237us/step - loss: 0.2517 - acc: 0.9058 - val_loss: 0.2930 - val_acc: 0.8927\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.2435 - acc: 0.9097 - val_loss: 0.3244 - val_acc: 0.8847\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.2319 - acc: 0.9140 - val_loss: 0.2994 - val_acc: 0.8918\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.2239 - acc: 0.9167 - val_loss: 0.3030 - val_acc: 0.8932\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 0.2119 - acc: 0.9211 - val_loss: 0.2924 - val_acc: 0.8992\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.2071 - acc: 0.9227 - val_loss: 0.3008 - val_acc: 0.8962\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 9s 183us/step - loss: 0.1967 - acc: 0.9277 - val_loss: 0.3069 - val_acc: 0.8944\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.1882 - acc: 0.9286 - val_loss: 0.3059 - val_acc: 0.8934\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 9s 178us/step - loss: 0.1825 - acc: 0.9317 - val_loss: 0.2902 - val_acc: 0.9010\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 0.1762 - acc: 0.9344 - val_loss: 0.2998 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 0.1693 - acc: 0.9359 - val_loss: 0.3140 - val_acc: 0.8956\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.1593 - acc: 0.9412 - val_loss: 0.3112 - val_acc: 0.8957\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 10s 200us/step - loss: 0.1580 - acc: 0.9415 - val_loss: 0.3035 - val_acc: 0.8972\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 10s 210us/step - loss: 0.1537 - acc: 0.9423 - val_loss: 0.2978 - val_acc: 0.9013\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 10s 204us/step - loss: 0.1444 - acc: 0.9459 - val_loss: 0.3081 - val_acc: 0.9008\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 10s 204us/step - loss: 0.1413 - acc: 0.9475 - val_loss: 0.2939 - val_acc: 0.9009\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 10s 206us/step - loss: 0.1374 - acc: 0.9490 - val_loss: 0.3015 - val_acc: 0.9040\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 8s 177us/step - loss: 0.1332 - acc: 0.9508 - val_loss: 0.3127 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.1265 - acc: 0.9528 - val_loss: 0.3282 - val_acc: 0.8953\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 9s 188us/step - loss: 0.1220 - acc: 0.9547 - val_loss: 0.3310 - val_acc: 0.9017\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 10s 210us/step - loss: 0.1183 - acc: 0.9566 - val_loss: 0.3207 - val_acc: 0.8973\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 8s 176us/step - loss: 0.1149 - acc: 0.9575 - val_loss: 0.3349 - val_acc: 0.8950\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 8s 173us/step - loss: 0.1091 - acc: 0.9605 - val_loss: 0.3354 - val_acc: 0.8979\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 8s 171us/step - loss: 0.1097 - acc: 0.9595 - val_loss: 0.3448 - val_acc: 0.8962\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.1081 - acc: 0.9586 - val_loss: 0.3394 - val_acc: 0.9030\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 11s 220us/step - loss: 0.0997 - acc: 0.9622 - val_loss: 0.3409 - val_acc: 0.8991\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 10s 218us/step - loss: 0.0996 - acc: 0.9629 - val_loss: 0.3328 - val_acc: 0.9029\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.0929 - acc: 0.9658 - val_loss: 0.3389 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.0906 - acc: 0.9661 - val_loss: 0.3576 - val_acc: 0.9035\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.0878 - acc: 0.9671 - val_loss: 0.3558 - val_acc: 0.9008\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 8s 177us/step - loss: 0.0981 - acc: 0.9635 - val_loss: 0.3588 - val_acc: 0.8986\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.0853 - acc: 0.9685 - val_loss: 0.3502 - val_acc: 0.9012\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 9s 178us/step - loss: 0.0763 - acc: 0.9719 - val_loss: 0.3625 - val_acc: 0.9048\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 8s 177us/step - loss: 0.0810 - acc: 0.9701 - val_loss: 0.3754 - val_acc: 0.8986\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 9s 183us/step - loss: 0.0787 - acc: 0.9711 - val_loss: 0.3843 - val_acc: 0.8974\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.0722 - acc: 0.9734 - val_loss: 0.3919 - val_acc: 0.9029\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.0751 - acc: 0.9722 - val_loss: 0.4025 - val_acc: 0.8978\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.0680 - acc: 0.9749 - val_loss: 0.3905 - val_acc: 0.9007\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.0705 - acc: 0.9737 - val_loss: 0.3867 - val_acc: 0.9024\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 9s 183us/step - loss: 0.0686 - acc: 0.9746 - val_loss: 0.4030 - val_acc: 0.8982\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.0689 - acc: 0.9741 - val_loss: 0.4131 - val_acc: 0.8968\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 10s 202us/step - loss: 0.0669 - acc: 0.9749 - val_loss: 0.4038 - val_acc: 0.8988\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 9s 198us/step - loss: 0.0594 - acc: 0.9777 - val_loss: 0.4323 - val_acc: 0.8949\n"
     ]
    }
   ],
   "source": [
    "model1_fit = model1.fit(X_train.reshape(-1, IMG_ROWS * IMG_COLS), y_train_cat, \n",
    "                        epochs = EPOCHS, batch_size = BATCH_SIZE, validation_data=(X_val.reshape(-1, IMG_ROWS * IMG_COLS), y_val_cat), \n",
    "                        callbacks=[tensorboard_callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  0.1590765226857364\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Loss: \", np.average(model1_fit.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_predict = model1.predict(X_test.reshape(-1, IMG_ROWS * IMG_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = []\n",
    "\n",
    "for index in range(0, len(temp_predict)):\n",
    "    predict1.append(np.argmax(temp_predict[index]))\n",
    "    \n",
    "predict1 = np.asarray(predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.82      0.87      0.84      1000\n",
      "     Trouser       0.99      0.99      0.99      1000\n",
      "    Pullover       0.84      0.82      0.83      1000\n",
      "       Dress       0.94      0.89      0.91      1000\n",
      "        Coat       0.88      0.77      0.82      1000\n",
      "      Sandal       0.99      0.95      0.97      1000\n",
      "       Shirt       0.68      0.78      0.73      1000\n",
      "     Sneaker       0.95      0.95      0.95      1000\n",
      "         Bag       0.98      0.98      0.98      1000\n",
      "  Ankle Boot       0.94      0.98      0.96      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.reshape(-1, 1), predict1.reshape(-1, 1), target_names=class_names.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. CNN without Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 32\n",
    "KERNEL_SIZE = 3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/train_data/\" + \"model2\"\n",
    "tensorboard_callback2 = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 04:05:43.589227 10392 deprecation_wrapper.py:119] From C:\\Users\\Ankit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential(name=\"model2\")\n",
    "model2.add(Conv2D(N_HIDDEN, kernel_size=(KERNEL_SIZE, KERNEL_SIZE), activation=\"relu\", kernel_initializer=\"he_normal\", \n",
    "                  input_shape=(IMG_ROWS, IMG_COLS, 1), name=\"layer1\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2), name=\"layer2\"))\n",
    "model2.add(Conv2D(N_HIDDEN * 2, kernel_size=(KERNEL_SIZE, KERNEL_SIZE), activation=\"relu\", name=\"layer3\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2), name=\"layer4\"))\n",
    "model2.add(Conv2D(N_HIDDEN * 4, kernel_size=(KERNEL_SIZE, KERNEL_SIZE), activation=\"relu\", name=\"layer5\"))\n",
    "model2.add(Flatten(name=\"layer6\"))\n",
    "model2.add(Dense(N_HIDDEN * 4, activation=\"relu\", name=\"layer7\"))\n",
    "model2.add(Dense(NUM_CLASSES, activation='softmax', name=\"layer8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "layer2 (MaxPooling2D)        (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "layer3 (Conv2D)              (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "layer4 (MaxPooling2D)        (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "layer5 (Conv2D)              (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "layer6 (Flatten)             (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "layer7 (Dense)               (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "layer8 (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 241,546\n",
      "Trainable params: 241,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 34s 710us/step - loss: 0.5359 - acc: 0.8034 - val_loss: 0.3832 - val_acc: 0.8611\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 36s 749us/step - loss: 0.3436 - acc: 0.8768 - val_loss: 0.3301 - val_acc: 0.8787\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 0.2973 - acc: 0.8905 - val_loss: 0.3211 - val_acc: 0.8843\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 35s 721us/step - loss: 0.2637 - acc: 0.9026 - val_loss: 0.2855 - val_acc: 0.8984\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 32s 659us/step - loss: 0.2395 - acc: 0.9120 - val_loss: 0.2775 - val_acc: 0.9001\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 34s 716us/step - loss: 0.2162 - acc: 0.9207 - val_loss: 0.2628 - val_acc: 0.9057\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 41s 860us/step - loss: 0.1984 - acc: 0.9265 - val_loss: 0.2661 - val_acc: 0.9038\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 37s 763us/step - loss: 0.1797 - acc: 0.9325 - val_loss: 0.2968 - val_acc: 0.8939\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 36s 758us/step - loss: 0.1635 - acc: 0.9395 - val_loss: 0.2658 - val_acc: 0.9078\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 37s 772us/step - loss: 0.1450 - acc: 0.9459 - val_loss: 0.2671 - val_acc: 0.9116\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 36s 751us/step - loss: 0.1317 - acc: 0.9517 - val_loss: 0.2749 - val_acc: 0.9110- ETA\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 35s 735us/step - loss: 0.1194 - acc: 0.9550 - val_loss: 0.2902 - val_acc: 0.91260.1189 - acc: 0.955 - ET\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 37s 764us/step - loss: 0.1085 - acc: 0.9589 - val_loss: 0.2851 - val_acc: 0.9122\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 37s 769us/step - loss: 0.0936 - acc: 0.9656 - val_loss: 0.2926 - val_acc: 0.9110\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 34s 701us/step - loss: 0.0825 - acc: 0.9687 - val_loss: 0.3248 - val_acc: 0.9068- ETA: 0s - loss: 0.0822 - acc\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 36s 746us/step - loss: 0.0758 - acc: 0.9723 - val_loss: 0.3203 - val_acc: 0.9057\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 33s 685us/step - loss: 0.0709 - acc: 0.9734 - val_loss: 0.3463 - val_acc: 0.9113\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 34s 717us/step - loss: 0.0587 - acc: 0.9785 - val_loss: 0.3475 - val_acc: 0.9110\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 33s 687us/step - loss: 0.0474 - acc: 0.9829 - val_loss: 0.3736 - val_acc: 0.9084\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 33s 697us/step - loss: 0.0455 - acc: 0.9838 - val_loss: 0.3982 - val_acc: 0.9051\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 0.0486 - acc: 0.9818 - val_loss: 0.4233 - val_acc: 0.9089\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 33s 695us/step - loss: 0.0461 - acc: 0.9827 - val_loss: 0.4012 - val_acc: 0.9082\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 34s 703us/step - loss: 0.0379 - acc: 0.9858 - val_loss: 0.4154 - val_acc: 0.9111\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 37s 763us/step - loss: 0.0333 - acc: 0.9878 - val_loss: 0.4355 - val_acc: 0.9115\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 35s 731us/step - loss: 0.0353 - acc: 0.9867 - val_loss: 0.4612 - val_acc: 0.9103\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 31s 653us/step - loss: 0.0318 - acc: 0.9880 - val_loss: 0.4454 - val_acc: 0.9107\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 33s 681us/step - loss: 0.0276 - acc: 0.9904 - val_loss: 0.4509 - val_acc: 0.9108\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 31s 654us/step - loss: 0.0262 - acc: 0.9904 - val_loss: 0.4962 - val_acc: 0.9071\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 33s 685us/step - loss: 0.0250 - acc: 0.9918 - val_loss: 0.5013 - val_acc: 0.9098\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 36s 747us/step - loss: 0.0241 - acc: 0.9914 - val_loss: 0.4954 - val_acc: 0.9107\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 0.0231 - acc: 0.9920 - val_loss: 0.5059 - val_acc: 0.9086\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 34s 712us/step - loss: 0.0269 - acc: 0.9906 - val_loss: 0.5538 - val_acc: 0.9090\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 35s 724us/step - loss: 0.0243 - acc: 0.9910 - val_loss: 0.5312 - val_acc: 0.9093\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 0.0236 - acc: 0.9921 - val_loss: 0.5380 - val_acc: 0.9117\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 30s 630us/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.5824 - val_acc: 0.9073\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 31s 636us/step - loss: 0.0180 - acc: 0.9934 - val_loss: 0.5991 - val_acc: 0.9067\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 31s 653us/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.5550 - val_acc: 0.9060\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 32s 665us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.5873 - val_acc: 0.9054\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 31s 655us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 0.5765 - val_acc: 0.9096\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 32s 657us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.5858 - val_acc: 0.9078\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 0.0213 - acc: 0.9924 - val_loss: 0.5756 - val_acc: 0.9077\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 34s 712us/step - loss: 0.0167 - acc: 0.9941 - val_loss: 0.6118 - val_acc: 0.9089\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 32s 662us/step - loss: 0.0166 - acc: 0.9939 - val_loss: 0.6156 - val_acc: 0.9095\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 32s 668us/step - loss: 0.0182 - acc: 0.9935 - val_loss: 0.6455 - val_acc: 0.9063\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 0.0228 - acc: 0.9919 - val_loss: 0.6363 - val_acc: 0.9058\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 32s 659us/step - loss: 0.0118 - acc: 0.9955 - val_loss: 0.6209 - val_acc: 0.9089\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 32s 664us/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.5987 - val_acc: 0.9092\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 32s 657us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.6130 - val_acc: 0.9084\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 32s 658us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.6451 - val_acc: 0.9042\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 31s 652us/step - loss: 0.0173 - acc: 0.9942 - val_loss: 0.6339 - val_acc: 0.9106\n"
     ]
    }
   ],
   "source": [
    "model2_fit = model2.fit(X_train, y_train_cat, \n",
    "                        epochs = EPOCHS, batch_size = BATCH_SIZE, validation_data=(X_val, y_val_cat), \n",
    "                        callbacks=[tensorboard_callback2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  0.08248990760542335\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Loss: \", np.average(model2_fit.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_predict = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = []\n",
    "\n",
    "for index in range(0, len(temp_predict)):\n",
    "    predict2.append(np.argmax(temp_predict[index]))\n",
    "    \n",
    "predict2 = np.asarray(predict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.81      0.90      0.85      1000\n",
      "     Trouser       0.97      0.99      0.98      1000\n",
      "    Pullover       0.92      0.79      0.85      1000\n",
      "       Dress       0.90      0.93      0.92      1000\n",
      "        Coat       0.87      0.88      0.87      1000\n",
      "      Sandal       0.99      0.97      0.98      1000\n",
      "       Shirt       0.76      0.75      0.75      1000\n",
      "     Sneaker       0.95      0.97      0.96      1000\n",
      "         Bag       0.99      0.98      0.98      1000\n",
      "  Ankle Boot       0.96      0.97      0.97      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.reshape(-1, 1), predict2.reshape(-1, 1), target_names=class_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
